{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reeceshuttleworth/Dropbox/Mac (3)/Documents/GitHub/958/958/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import inspect\n",
    "\n",
    "torch.set_default_device(\"cpu\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\n",
    "\n",
    "inputs = tokenizer('''```python\n",
    "def print_prime(n):\n",
    "   \"\"\"\n",
    "   Print all primes between 1 and n\n",
    "   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def print_prime(n):\n",
      "   \"\"\"\n",
      "   Print all primes between 1 and n\n",
      "   \"\"\"\n",
      "   primes = []\n",
      "   for num in range(2, n+1):\n",
      "       is_prime = True\n",
      "       for i in range(2, int(num**0.5)+1):\n",
      "           if num % i == 0:\n",
      "               is_prime = False\n",
      "               break\n",
      "       if is_prime:\n",
      "           primes.append(num)\n",
      "   print(primes)\n",
      "\n",
      "print_prime(20)\n",
      "```\n",
      "\n",
      "## Exercises\n",
      "\n",
      "1. Write a Python function that takes a list of numbers and returns the sum of all even numbers in the list.\n",
      "\n",
      "```python\n",
      "def sum_even(numbers):\n",
      "   \"\"\"\n",
      "   Returns the sum of all even numbers in the list\n",
      "   \"\"\"\n",
      "   return sum(num for num in numbers if\n"
     ]
    }
   ],
   "source": [
    "before_edit_outputs = model.generate(**inputs, max_length=200)\n",
    "# print(before_edit_outputs)\n",
    "be_text = tokenizer.batch_decode(before_edit_outputs)[0]\n",
    "print(be_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def forward(self, input_ids: torch.LongTensor) -> torch.FloatTensor:\n",
      "        input_shape = input_ids.size()\n",
      "        input_ids = input_ids.view(-1, input_shape[-1])\n",
      "\n",
      "        hidden_states = self.wte(input_ids)\n",
      "        hidden_states = self.drop(hidden_states)\n",
      "\n",
      "        return hidden_states\n",
      "\n",
      "def new_forward(self, input_ids: torch.LongTensor) -> torch.FloatTensor:\n",
      "    input_shape = input_ids.size()\n",
      "    input_ids = input_ids.view(-1, input_shape[-1])\n",
      "    print(input_ids)\n",
      "\n",
      "    hidden_states = self.wte(input_ids)\n",
      "    hidden_states = self.drop(hidden_states)\n",
      "\n",
      "    return hidden_states\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(model.layers[0].forward))\n",
    "\n",
    "def new_forward(self, input_ids: torch.LongTensor) -> torch.FloatTensor:\n",
    "    input_shape = input_ids.size()\n",
    "    input_ids = input_ids.view(-1, input_shape[-1])\n",
    "    print(input_ids)\n",
    "\n",
    "    hidden_states = self.wte(input_ids)\n",
    "    hidden_states = self.drop(hidden_states)\n",
    "\n",
    "    return hidden_states\n",
    "\n",
    "# this didn't work:\n",
    "# model.layers[0].forward = forward\n",
    "\n",
    "# from https://discuss.pytorch.org/t/how-can-i-replace-the-forward-method-of-a-predefined-torchvision-model-with-my-customized-forward-function/54224/10\n",
    "# (carmelo calafiore's response)\n",
    "desired_layer = model.layers[0]\n",
    "bound_method = new_forward.__get__(desired_layer, desired_layer.__class__)\n",
    "setattr(desired_layer, 'forward', bound_method)\n",
    "\n",
    "\n",
    "print(inspect.getsource(model.layers[0].forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15506,    63, 29412,   198,  4299,  3601,    62, 35505,     7,    77,\n",
      "          2599,   198, 50285, 37811,   198, 50285, 18557,   477,   778,   999,\n",
      "          1022,   352,   290,   299,   198, 50285, 37811]])\n",
      "tensor([[198]])\n",
      "tensor([[50285]])\n",
      "tensor([[1050]])\n",
      "tensor([[999]])\n",
      "tensor([[796]])\n",
      "tensor([[17635]])\n",
      "tensor([[198]])\n",
      "tensor([[50285]])\n",
      "tensor([[1640]])\n",
      "tensor([[997]])\n",
      "tensor([[287]])\n",
      "tensor([[2837]])\n",
      "tensor([[7]])\n",
      "tensor([[17]])\n",
      "tensor([[11]])\n",
      "tensor([[299]])\n",
      "tensor([[10]])\n",
      "tensor([[16]])\n",
      "tensor([[2599]])\n",
      "tensor([[198]])\n",
      "tensor([[50281]])\n",
      "tensor([[271]])\n",
      "tensor([[62]])\n",
      "tensor([[35505]])\n",
      "tensor([[796]])\n",
      "tensor([[6407]])\n",
      "tensor([[198]])\n",
      "tensor([[50281]])\n",
      "tensor([[1640]])\n",
      "tensor([[1312]])\n",
      "tensor([[287]])\n",
      "tensor([[2837]])\n",
      "tensor([[7]])\n",
      "tensor([[17]])\n",
      "tensor([[11]])\n",
      "tensor([[493]])\n",
      "tensor([[7]])\n",
      "tensor([[22510]])\n",
      "tensor([[1174]])\n",
      "tensor([[15]])\n",
      "tensor([[13]])\n",
      "tensor([[20]])\n",
      "tensor([[47762]])\n",
      "tensor([[16]])\n",
      "tensor([[2599]])\n",
      "tensor([[198]])\n",
      "tensor([[50277]])\n",
      "tensor([[361]])\n",
      "tensor([[997]])\n",
      "tensor([[4064]])\n",
      "tensor([[1312]])\n",
      "tensor([[6624]])\n",
      "tensor([[657]])\n",
      "tensor([[25]])\n",
      "tensor([[198]])\n",
      "tensor([[50273]])\n",
      "tensor([[271]])\n",
      "tensor([[62]])\n",
      "tensor([[35505]])\n",
      "tensor([[796]])\n",
      "tensor([[10352]])\n",
      "tensor([[198]])\n",
      "tensor([[50273]])\n",
      "tensor([[9032]])\n",
      "tensor([[198]])\n",
      "tensor([[50281]])\n",
      "tensor([[361]])\n",
      "tensor([[318]])\n",
      "tensor([[62]])\n",
      "tensor([[35505]])\n",
      "tensor([[25]])\n",
      "tensor([[198]])\n",
      "tensor([[50277]])\n",
      "tensor([[1050]])\n",
      "tensor([[999]])\n",
      "tensor([[13]])\n",
      "tensor([[33295]])\n",
      "tensor([[7]])\n",
      "tensor([[22510]])\n",
      "tensor([[8]])\n",
      "tensor([[198]])\n",
      "tensor([[50285]])\n",
      "tensor([[4798]])\n",
      "tensor([[7]])\n",
      "tensor([[1050]])\n",
      "tensor([[999]])\n",
      "tensor([[8]])\n",
      "tensor([[198]])\n",
      "tensor([[198]])\n",
      "tensor([[4798]])\n",
      "tensor([[62]])\n",
      "tensor([[35505]])\n",
      "tensor([[7]])\n",
      "tensor([[1238]])\n",
      "tensor([[8]])\n",
      "tensor([[198]])\n",
      "tensor([[15506]])\n",
      "tensor([[63]])\n",
      "tensor([[198]])\n",
      "tensor([[198]])\n",
      "tensor([[2235]])\n",
      "tensor([[1475]])\n",
      "tensor([[2798]])\n",
      "tensor([[2696]])\n",
      "tensor([[198]])\n",
      "tensor([[198]])\n",
      "tensor([[16]])\n",
      "tensor([[13]])\n",
      "tensor([[19430]])\n",
      "tensor([[257]])\n",
      "tensor([[11361]])\n",
      "tensor([[2163]])\n",
      "tensor([[326]])\n",
      "tensor([[2753]])\n",
      "tensor([[257]])\n",
      "tensor([[1351]])\n",
      "tensor([[286]])\n",
      "tensor([[3146]])\n",
      "tensor([[290]])\n",
      "tensor([[5860]])\n",
      "tensor([[262]])\n",
      "tensor([[2160]])\n",
      "tensor([[286]])\n",
      "tensor([[477]])\n",
      "tensor([[772]])\n",
      "tensor([[3146]])\n",
      "tensor([[287]])\n",
      "tensor([[262]])\n",
      "tensor([[1351]])\n",
      "tensor([[13]])\n",
      "tensor([[198]])\n",
      "tensor([[198]])\n",
      "tensor([[15506]])\n",
      "tensor([[63]])\n",
      "tensor([[29412]])\n",
      "tensor([[198]])\n",
      "tensor([[4299]])\n",
      "tensor([[2160]])\n",
      "tensor([[62]])\n",
      "tensor([[10197]])\n",
      "tensor([[7]])\n",
      "tensor([[77]])\n",
      "tensor([[17024]])\n",
      "tensor([[2599]])\n",
      "tensor([[198]])\n",
      "tensor([[50285]])\n",
      "tensor([[37811]])\n",
      "tensor([[198]])\n",
      "tensor([[50285]])\n",
      "tensor([[35561]])\n",
      "tensor([[262]])\n",
      "tensor([[2160]])\n",
      "tensor([[286]])\n",
      "tensor([[477]])\n",
      "tensor([[772]])\n",
      "tensor([[3146]])\n",
      "tensor([[287]])\n",
      "tensor([[262]])\n",
      "tensor([[1351]])\n",
      "tensor([[198]])\n",
      "tensor([[50285]])\n",
      "tensor([[37811]])\n",
      "tensor([[198]])\n",
      "tensor([[50285]])\n",
      "tensor([[7783]])\n",
      "tensor([[2160]])\n",
      "tensor([[7]])\n",
      "tensor([[22510]])\n",
      "tensor([[329]])\n",
      "tensor([[997]])\n",
      "tensor([[287]])\n",
      "tensor([[3146]])\n",
      "```python\n",
      "def print_prime(n):\n",
      "   \"\"\"\n",
      "   Print all primes between 1 and n\n",
      "   \"\"\"\n",
      "   primes = []\n",
      "   for num in range(2, n+1):\n",
      "       is_prime = True\n",
      "       for i in range(2, int(num**0.5)+1):\n",
      "           if num % i == 0:\n",
      "               is_prime = False\n",
      "               break\n",
      "       if is_prime:\n",
      "           primes.append(num)\n",
      "   print(primes)\n",
      "\n",
      "print_prime(20)\n",
      "```\n",
      "\n",
      "## Exercises\n",
      "\n",
      "1. Write a Python function that takes a list of numbers and returns the sum of all even numbers in the list.\n",
      "\n",
      "```python\n",
      "def sum_even(numbers):\n",
      "   \"\"\"\n",
      "   Returns the sum of all even numbers in the list\n",
      "   \"\"\"\n",
      "   return sum(num for num in numbers if\n"
     ]
    }
   ],
   "source": [
    "after_edit_outputs = model.generate(**inputs, max_length=200)\n",
    "# print(after_edit_outputs)\n",
    "text = tokenizer.batch_decode(after_edit_outputs)[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output2 = model.forward(**inputs)\n",
    "# output2.logits.shape\n",
    "# inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def new_forward(self, input_ids: torch.LongTensor) -> torch.FloatTensor:\n",
      "    input_shape = input_ids.size()\n",
      "    input_ids = input_ids.view(-1, input_shape[-1])\n",
      "    print(input_ids)\n",
      "\n",
      "    hidden_states = self.wte(input_ids)\n",
      "    hidden_states = self.drop(hidden_states)\n",
      "\n",
      "    return hidden_states\n",
      "\n",
      "def new_forward(self, input_ids: torch.LongTensor) -> torch.FloatTensor:\n",
      "    input_shape = input_ids.size()\n",
      "    input_ids = input_ids.view(-1, input_shape[-1])\n",
      "    print(input_ids)\n",
      "\n",
      "    hidden_states = self.wte(input_ids)\n",
      "    hidden_states = self.drop(hidden_states)\n",
      "\n",
      "    return hidden_states\n",
      "\n"
     ]
    }
   ],
   "source": [
    "code1 = inspect.getsource(model.forward)\n",
    "code2 = inspect.getsource(model.layers[1].mixer.forward)\n",
    "# print(inspect.getsource(model.post_init))\n",
    "\n",
    "# print(inspect.getsource(model.forward)) # forward of entire model\n",
    "# print(inspect.getsource(model.generate))\n",
    "print(inspect.getsource(model.layers[0].forward))\n",
    "# print(inspect.getsource(model.layers[1].mixer.forward))\n",
    "\n",
    "# def new_forward(self, input_ids: torch.LongTensor) -> torch.FloatTensor:\n",
    "#     input_shape = input_ids.size()\n",
    "#     print(input_shape)\n",
    "#     input_ids = input_ids.view(-1, input_shape[-1])\n",
    "\n",
    "#     hidden_states = self.wte(input_ids)\n",
    "#     hidden_states = self.drop(hidden_states)\n",
    "\n",
    "#     return hidden_states\n",
    "\n",
    "# model.layers[0].forward = new_forward\n",
    "\n",
    "print(inspect.getsource(model.layers[0].forward))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15506,    63, 29412,   198,  4299,  3601,    62, 35505,     7,    77,\n",
      "          2599,   198, 50285, 37811,   198, 50285, 18557,   477,   778,   999,\n",
      "          1022,   352,   290,   299,   198, 50285, 37811]])\n",
      "torch.Size([1, 27])\n",
      "torch.Size([1, 27, 2048])\n"
     ]
    }
   ],
   "source": [
    "out_first_layer = model.layers[0].forward(**inputs)\n",
    "print(inputs.input_ids.shape)\n",
    "print(out_first_layer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Embedding(\n",
       "    (wte): Embedding(51200, 2048)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (1): ParallelBlock(\n",
       "    (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (mixer): MHA(\n",
       "      (rotary_emb): RotaryEmbedding()\n",
       "      (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "      (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (inner_attn): SelfAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (inner_cross_attn): CrossAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (mlp): MLP(\n",
       "      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "      (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "      (act): NewGELUActivation()\n",
       "    )\n",
       "  )\n",
       "  (2): ParallelBlock(\n",
       "    (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (mixer): MHA(\n",
       "      (rotary_emb): RotaryEmbedding()\n",
       "      (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "      (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (inner_attn): SelfAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (inner_cross_attn): CrossAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (mlp): MLP(\n",
       "      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "      (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "      (act): NewGELUActivation()\n",
       "    )\n",
       "  )\n",
       "  (3): ParallelBlock(\n",
       "    (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (mixer): MHA(\n",
       "      (rotary_emb): RotaryEmbedding()\n",
       "      (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "      (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (inner_attn): SelfAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (inner_cross_attn): CrossAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (mlp): MLP(\n",
       "      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "      (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "      (act): NewGELUActivation()\n",
       "    )\n",
       "  )\n",
       "  (4): ParallelBlock(\n",
       "    (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (mixer): MHA(\n",
       "      (rotary_emb): RotaryEmbedding()\n",
       "      (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "      (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (inner_attn): SelfAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (inner_cross_attn): CrossAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (mlp): MLP(\n",
       "      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "      (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "      (act): NewGELUActivation()\n",
       "    )\n",
       "  )\n",
       "  (5): ParallelBlock(\n",
       "    (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (mixer): MHA(\n",
       "      (rotary_emb): RotaryEmbedding()\n",
       "      (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "      (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (inner_attn): SelfAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (inner_cross_attn): CrossAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (mlp): MLP(\n",
       "      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "      (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "      (act): NewGELUActivation()\n",
       "    )\n",
       "  )\n",
       "  (6): ParallelBlock(\n",
       "    (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (mixer): MHA(\n",
       "      (rotary_emb): RotaryEmbedding()\n",
       "      (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "      (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (inner_attn): SelfAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (inner_cross_attn): CrossAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (mlp): MLP(\n",
       "      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "      (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "      (act): NewGELUActivation()\n",
       "    )\n",
       "  )\n",
       "  (7): ParallelBlock(\n",
       "    (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (mixer): MHA(\n",
       "      (rotary_emb): RotaryEmbedding()\n",
       "      (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "      (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (inner_attn): SelfAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (inner_cross_attn): CrossAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (mlp): MLP(\n",
       "      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "      (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "      (act): NewGELUActivation()\n",
       "    )\n",
       "  )\n",
       "  (8): ParallelBlock(\n",
       "    (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (mixer): MHA(\n",
       "      (rotary_emb): RotaryEmbedding()\n",
       "      (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "      (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (inner_attn): SelfAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (inner_cross_attn): CrossAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (mlp): MLP(\n",
       "      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "      (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "      (act): NewGELUActivation()\n",
       "    )\n",
       "  )\n",
       "  (9): ParallelBlock(\n",
       "    (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (mixer): MHA(\n",
       "      (rotary_emb): RotaryEmbedding()\n",
       "      (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "      (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (inner_attn): SelfAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (inner_cross_attn): CrossAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (mlp): MLP(\n",
       "      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "      (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "      (act): NewGELUActivation()\n",
       "    )\n",
       "  )\n",
       "  (10): ParallelBlock(\n",
       "    (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (mixer): MHA(\n",
       "      (rotary_emb): RotaryEmbedding()\n",
       "      (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "      (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (inner_attn): SelfAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (inner_cross_attn): CrossAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (mlp): MLP(\n",
       "      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "      (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "      (act): NewGELUActivation()\n",
       "    )\n",
       "  )\n",
       "  (11): ParallelBlock(\n",
       "    (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (mixer): MHA(\n",
       "      (rotary_emb): RotaryEmbedding()\n",
       "      (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "      (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (inner_attn): SelfAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (inner_cross_attn): CrossAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (mlp): MLP(\n",
       "      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "      (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "      (act): NewGELUActivation()\n",
       "    )\n",
       "  )\n",
       "  (12): ParallelBlock(\n",
       "    (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (mixer): MHA(\n",
       "      (rotary_emb): RotaryEmbedding()\n",
       "      (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "      (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (inner_attn): SelfAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (inner_cross_attn): CrossAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (mlp): MLP(\n",
       "      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "      (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "      (act): NewGELUActivation()\n",
       "    )\n",
       "  )\n",
       "  (13): ParallelBlock(\n",
       "    (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (mixer): MHA(\n",
       "      (rotary_emb): RotaryEmbedding()\n",
       "      (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "      (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (inner_attn): SelfAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (inner_cross_attn): CrossAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (mlp): MLP(\n",
       "      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "      (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "      (act): NewGELUActivation()\n",
       "    )\n",
       "  )\n",
       "  (14): ParallelBlock(\n",
       "    (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (mixer): MHA(\n",
       "      (rotary_emb): RotaryEmbedding()\n",
       "      (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "      (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (inner_attn): SelfAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (inner_cross_attn): CrossAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (mlp): MLP(\n",
       "      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "      (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "      (act): NewGELUActivation()\n",
       "    )\n",
       "  )\n",
       "  (15): ParallelBlock(\n",
       "    (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (mixer): MHA(\n",
       "      (rotary_emb): RotaryEmbedding()\n",
       "      (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "      (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (inner_attn): SelfAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (inner_cross_attn): CrossAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (mlp): MLP(\n",
       "      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "      (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "      (act): NewGELUActivation()\n",
       "    )\n",
       "  )\n",
       "  (16): ParallelBlock(\n",
       "    (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (mixer): MHA(\n",
       "      (rotary_emb): RotaryEmbedding()\n",
       "      (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "      (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (inner_attn): SelfAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (inner_cross_attn): CrossAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (mlp): MLP(\n",
       "      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "      (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "      (act): NewGELUActivation()\n",
       "    )\n",
       "  )\n",
       "  (17): ParallelBlock(\n",
       "    (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (mixer): MHA(\n",
       "      (rotary_emb): RotaryEmbedding()\n",
       "      (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "      (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (inner_attn): SelfAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (inner_cross_attn): CrossAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (mlp): MLP(\n",
       "      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "      (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "      (act): NewGELUActivation()\n",
       "    )\n",
       "  )\n",
       "  (18): ParallelBlock(\n",
       "    (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (mixer): MHA(\n",
       "      (rotary_emb): RotaryEmbedding()\n",
       "      (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "      (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (inner_attn): SelfAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (inner_cross_attn): CrossAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (mlp): MLP(\n",
       "      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "      (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "      (act): NewGELUActivation()\n",
       "    )\n",
       "  )\n",
       "  (19): ParallelBlock(\n",
       "    (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (mixer): MHA(\n",
       "      (rotary_emb): RotaryEmbedding()\n",
       "      (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "      (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (inner_attn): SelfAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (inner_cross_attn): CrossAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (mlp): MLP(\n",
       "      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "      (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "      (act): NewGELUActivation()\n",
       "    )\n",
       "  )\n",
       "  (20): ParallelBlock(\n",
       "    (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (mixer): MHA(\n",
       "      (rotary_emb): RotaryEmbedding()\n",
       "      (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "      (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (inner_attn): SelfAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (inner_cross_attn): CrossAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (mlp): MLP(\n",
       "      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "      (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "      (act): NewGELUActivation()\n",
       "    )\n",
       "  )\n",
       "  (21): ParallelBlock(\n",
       "    (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (mixer): MHA(\n",
       "      (rotary_emb): RotaryEmbedding()\n",
       "      (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "      (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (inner_attn): SelfAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (inner_cross_attn): CrossAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (mlp): MLP(\n",
       "      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "      (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "      (act): NewGELUActivation()\n",
       "    )\n",
       "  )\n",
       "  (22): ParallelBlock(\n",
       "    (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (mixer): MHA(\n",
       "      (rotary_emb): RotaryEmbedding()\n",
       "      (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "      (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (inner_attn): SelfAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (inner_cross_attn): CrossAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (mlp): MLP(\n",
       "      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "      (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "      (act): NewGELUActivation()\n",
       "    )\n",
       "  )\n",
       "  (23): ParallelBlock(\n",
       "    (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (mixer): MHA(\n",
       "      (rotary_emb): RotaryEmbedding()\n",
       "      (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "      (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (inner_attn): SelfAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (inner_cross_attn): CrossAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (mlp): MLP(\n",
       "      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "      (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "      (act): NewGELUActivation()\n",
       "    )\n",
       "  )\n",
       "  (24): ParallelBlock(\n",
       "    (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (mixer): MHA(\n",
       "      (rotary_emb): RotaryEmbedding()\n",
       "      (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "      (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (inner_attn): SelfAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (inner_cross_attn): CrossAttention(\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (mlp): MLP(\n",
       "      (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "      (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "      (act): NewGELUActivation()\n",
       "    )\n",
       "  )\n",
       "  (25): CausalLMHead(\n",
       "    (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (linear): Linear(in_features=2048, out_features=51200, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "958",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
